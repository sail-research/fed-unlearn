{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Cases:\n",
    "        Case 0: normal federated learning\n",
    "        Case 1: baseline, retrain from scratch\n",
    "        Case 2: method 1: continue train\n",
    "        Case 3: method 2: PGA\n",
    "        Case 4: method 3: federaser\n",
    "        Case 5: method 4: flipping\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "    List of settings:\n",
    "    1. MNIST: \n",
    "        - R10, UR5, PR15, OR15\n",
    "        - R10, UR1, PR15, OR15\n",
    "        - R50, UR5, PR15, OR15\n",
    "    2. CIFAR10\n",
    "        - R20, UR10, PR30, OR30\n",
    "        - R20, UR2, PR30, OR30\n",
    "        - R100, UR10, PR30, OR30\n",
    "    List of experiments:\n",
    "    1. Accuracy\n",
    "        - compare case 2 with case 1\n",
    "        - compare case 3 with case 1\n",
    "        - compare case 4 with case 1\n",
    "        - compare case 5 with case 1\n",
    "    2. Accuracy on the last round before onboarding\n",
    "    3. Params similarity\n",
    "    4. Prediction Similarity\n",
    "    5. Unlearning time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "from utils.model import get_model\n",
    "import torch\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configs for experiments\n",
    "\n",
    "configs = {\n",
    "    \"mnist\": {\n",
    "        \"num_round\": 50,\n",
    "        \"num_unlearn_round\": 5,\n",
    "        \"num_post_training_round\": 15\n",
    "    },\n",
    "    \"cifar10\": {\n",
    "        \"num_round\": 100,\n",
    "        \"num_unlearn_round\": 10,\n",
    "        \"num_post_training_round\": 30\n",
    "    },\n",
    "    \"cifar100\": {\n",
    "        \"num_round\": 100,\n",
    "        \"num_unlearn_round\": 10,\n",
    "        \"num_post_training_round\": 30\n",
    "    },\n",
    "    \"dataset\": \"cifar100\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result structure\n",
    "res = {}\n",
    "\n",
    "for k1 in (\"train\", \"val\"):\n",
    "    res[k1] = {}\n",
    "    for k2 in (\"loss\", \"acc\"):\n",
    "        res[k1][k2] = {}\n",
    "        res[k1][k2][\"avg\"] = []\n",
    "        res[k1][k2][\"clean\"] = []\n",
    "        res[k1][k2][\"backdoor\"] = []\n",
    "        for k3 in range(5):\n",
    "            res[k1][k2][k3] = []\n",
    "\n",
    "# or, for better visualization, this is the architecture of res\n",
    "\n",
    "res = {\n",
    "    \"train\": {\n",
    "        \"loss\": {\n",
    "            \"avg\": [],\n",
    "            \"clean\": [],\n",
    "            \"backdoor\": [],\n",
    "            0: [],\n",
    "            1: [],\n",
    "            2: [],\n",
    "            3: [],\n",
    "            4: []\n",
    "        },\n",
    "        \"acc\": {\n",
    "            \"avg\": [],\n",
    "            \"clean\": [],\n",
    "            \"backdoor\": [],\n",
    "            0: [],\n",
    "            1: [],\n",
    "            2: [],\n",
    "            3: [],\n",
    "            4: []\n",
    "        }\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"loss\": {\n",
    "            \"avg\": [],\n",
    "            \"clean\": [],\n",
    "            \"backdoor\": [],\n",
    "            0: [],\n",
    "            1: [],\n",
    "            2: [],\n",
    "            3: [],\n",
    "            4: []\n",
    "        },\n",
    "        \"acc\": {\n",
    "            \"avg\": [],\n",
    "            \"clean\": [],\n",
    "            \"backdoor\": [],\n",
    "            0: [],\n",
    "            1: [],\n",
    "            2: [],\n",
    "            3: [],\n",
    "            4: []\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = os.listdir(\"with_onboarding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_csv(X,Ys, filename, is_cuda = False):\n",
    "    df = pd.DataFrame({\n",
    "        X[\"label\"]: X[\"value\"],\n",
    "    })\n",
    "\n",
    "    if is_cuda:\n",
    "        for label, Y in Ys.items():\n",
    "            df[label] = [y.cpu().item() for y in Y]\n",
    "    else:\n",
    "        for label, Y in Ys.items():\n",
    "            df[label] = Y\n",
    "\n",
    "    df.to_csv(\"csvs/\" + filename, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gen(filename, type=\"acc\"):\n",
    "    with open(filename, 'rb') as fp:\n",
    "        data = pickle.load(fp)['val'][type]\n",
    "        return data\n",
    "\n",
    "\n",
    "onboarding = True\n",
    "num_onboarding_rounds = 30\n",
    "\n",
    "\n",
    "if onboarding:\n",
    "    folder = \"with_onboarding/\"\n",
    "else:\n",
    "    folder = \"without_onboarding/\"\n",
    "\n",
    "\n",
    "name = {\n",
    "    \"case0\": \"normal\",\n",
    "    \"case1\": \"Retrain\",\n",
    "    \"case2\": \"Continue to Train\",\n",
    "    \"case3\": \"PGA\",\n",
    "    \"case4\": \"FedEraser\",\n",
    "    \"case5\": \"Flipping\"\n",
    "}\n",
    "\n",
    "\n",
    "def show_result(path, methods=[1, 2, 3, 4], is_marked=False):\n",
    "\n",
    "    markers = [\"\", \"bo--\", \"gx--\", \"m^-\", \"c+-\", \"r>-\", \"y<-\", \"ks-\", \"yd-\"]\n",
    "\n",
    "    num_rounds = 0\n",
    "\n",
    "    for i in [3, 4, 5]:\n",
    "        temp = 0\n",
    "        if i == 3:\n",
    "            temp = int(path.split(\"_\")[i][1:])\n",
    "        else:\n",
    "            temp = int(path.split(\"_\")[i][2:])\n",
    "\n",
    "        num_rounds += temp\n",
    "\n",
    "    num_rounds += num_onboarding_rounds    \n",
    "\n",
    "    fl_rounds = [i for i in range(1, num_rounds + 1)]\n",
    "\n",
    "    filename_baseline = f\"case0_{path}\"\n",
    "    baseline = load_gen(folder + filename_baseline)\n",
    "\n",
    "    for i in methods:\n",
    "        filename = f\"case{i}_{path}\"\n",
    "        try:\n",
    "            data = load_gen(folder + filename)\n",
    "        except:\n",
    "            continue\n",
    "        case = f\"case{i}\"\n",
    "\n",
    "        if i != 1:\n",
    "            clean_data = baseline[\"clean\"] + data[\"clean\"]\n",
    "            backdoor_data = baseline[\"backdoor\"] + data[\"backdoor\"]\n",
    "        else:\n",
    "            clean_data = data[\"clean\"]\n",
    "            backdoor_data = data[\"backdoor\"]\n",
    "\n",
    "        to_csv( \n",
    "            {\n",
    "                \"label\": \"Rounds\",\n",
    "                \"value\": fl_rounds\n",
    "            },\n",
    "            {\n",
    "                \"clean_data\": clean_data,\n",
    "                \"backdoor_data\": backdoor_data\n",
    "            },\n",
    "            f\"exp1_accuracy/{configs['dataset']}_case{i}_R{configs[configs['dataset']]['num_round']}_UR{configs[configs['dataset']]['num_unlearn_round']}_PR{configs[configs['dataset']]['num_post_training_round']}.csv\"\n",
    "        )\n",
    "\n",
    "        if is_marked:\n",
    "            plt.plot(fl_rounds, clean_data, markers[2*i-1], label=f\"{name[case]} clean\")\n",
    "            plt.plot(fl_rounds, backdoor_data, markers[2*i], label=f\"{name[case]} backdoor\")\n",
    "        else:\n",
    "            plt.plot(fl_rounds, clean_data, label=f\"{name[case]} clean\")\n",
    "            plt.plot(fl_rounds, backdoor_data, label=f\"{name[case]} backdoor\")\n",
    "\n",
    "    plt.xlabel('Rounds')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.locator_params(axis=\"x\", integer=True)\n",
    "    plt.grid()\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "    method_string = \"\"\n",
    "    for i in methods:\n",
    "        method_string += str(i)\n",
    "\n",
    "    plt.savefig(f\"plot/{configs['dataset']}/{path[:-4]}_M{method_string}.png\", dpi=1200, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_result_all(path, methods=[1, 2, 3, 4, 5], is_clean = True, is_marked=True):\n",
    "\n",
    "    markers = [\"\", \"^\", \"s\", \"<\", \"o\", \"v\"]\n",
    "    colors = [\"\", \"b\", \"orange\", \"g\", \"r\", \"k\"]\n",
    "\n",
    "    num_rounds = 0\n",
    "\n",
    "    for i in [3, 4, 5]:\n",
    "        temp = 0\n",
    "        if i == 3:\n",
    "            temp = int(path.split(\"_\")[i][1:])\n",
    "        else:\n",
    "            temp = int(path.split(\"_\")[i][2:])\n",
    "\n",
    "        num_rounds += temp\n",
    "\n",
    "    num_rounds += num_onboarding_rounds    \n",
    "\n",
    "    fl_rounds = [i for i in range(1, num_rounds + 1)]\n",
    "\n",
    "    filename_baseline = f\"case0_{path}\"\n",
    "    baseline = load_gen(folder + filename_baseline)\n",
    "\n",
    "    for i in methods:\n",
    "        filename = f\"case{i}_{path}\"\n",
    "        try:\n",
    "            data = load_gen(folder + filename)\n",
    "        except:\n",
    "            continue\n",
    "        case = f\"case{i}\"\n",
    "\n",
    "        if is_clean:\n",
    "            if i != 1:\n",
    "                clean_data = baseline[\"clean\"] + data[\"clean\"]\n",
    "                # backdoor_data = baseline[\"backdoor\"] + data[\"backdoor\"]\n",
    "            else:\n",
    "                clean_data = data[\"clean\"]\n",
    "                # backdoor_data = data[\"backdoor\"]\n",
    "\n",
    "            if is_marked:\n",
    "                plt.plot(fl_rounds, clean_data, marker = markers[i], markevery= 10, color = colors[i], label=f\"{name[case]}\")\n",
    "                # plt.plot(fl_rounds, backdoor_data, markers[2*i], label=f\"{name[case]} backdoor\")\n",
    "            else:\n",
    "                plt.plot(fl_rounds, clean_data, color = colors[i], label=f\"{name[case]}\")\n",
    "                # plt.plot(fl_rounds, backdoor_data, label=f\"{name[case]} backdoor\")\n",
    "        else:\n",
    "            if i != 1:\n",
    "                # clean_data = baseline[\"clean\"] + data[\"clean\"]\n",
    "                backdoor_data = baseline[\"backdoor\"] + data[\"backdoor\"]\n",
    "            else:\n",
    "                # clean_data = data[\"clean\"]\n",
    "                backdoor_data = data[\"backdoor\"]\n",
    "\n",
    "            if is_marked:\n",
    "                # plt.plot(fl_rounds, clean_data, markers[2*i-1], label=f\"{name[case]} clean\")\n",
    "                plt.plot(fl_rounds, backdoor_data, marker = markers[i], markevery=10, color = colors[i], label=f\"{name[case]}\")\n",
    "            else:\n",
    "                # plt.plot(fl_rounds, clean_data, label=f\"{name[case]} clean\")\n",
    "                plt.plot(fl_rounds, backdoor_data, color = colors[i], label=f\"{name[case]}\")\n",
    "\n",
    "    plt.xlabel('Rounds')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.locator_params(axis=\"x\", integer=True)\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best')\n",
    "\n",
    "    method_string = \"\"\n",
    "    for i in methods:\n",
    "        method_string += str(i)\n",
    "\n",
    "    type = \"\"\n",
    "    if is_clean:\n",
    "        type = \"clean\"\n",
    "    else:\n",
    "        type = \"backdoor\"\n",
    "\n",
    "    plt.savefig(f\"plot/{configs['dataset']}/{path[:-4]}_M{method_string}_{type}.png\", dpi=1200, bbox_inches='tight')\n",
    "    plt.savefig(f\"plot/{configs['dataset']}/{path[:-4]}_M{method_string}_{type}.pdf\", dpi=1200, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Accuracy of all methods\n",
    "\n",
    "path = f\"{configs['dataset']}_C5_BS128_R{configs[configs['dataset']]['num_round']}_UR{configs[configs['dataset']]['num_unlearn_round']}_PR{configs[configs['dataset']]['num_post_training_round']}_E1_LR0.01.pkl\"\n",
    "\n",
    "show_result_all(path, is_clean=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_last_round_result_before_onboarding(path, methods=[1, 2, 3, 4]):\n",
    "    filename_baseline = f\"case0_{path}\"\n",
    "    baseline = load_gen(folder + filename_baseline)\n",
    "\n",
    "    clean_data = []\n",
    "    backdoor_data = []\n",
    "\n",
    "    clean_labels = []\n",
    "    backdoor_labels = []\n",
    "\n",
    "    method_names = [name[f\"case{i}\"] for i in methods]\n",
    "    x_axis = np.arange(len(method_names))\n",
    "\n",
    "    for i in methods:\n",
    "        filename = f\"case{i}_{path}\"\n",
    "        try:\n",
    "            data = load_gen(folder + filename)\n",
    "        except:\n",
    "            continue\n",
    "        case = f\"case{i}\"\n",
    "\n",
    "\n",
    "        clean_data.append(data[\"clean\"][-configs[configs[\"dataset\"]][\"num_post_training_round\"]-1])\n",
    "        backdoor_data.append(data[\"backdoor\"][-configs[configs[\"dataset\"]][\"num_post_training_round\"]-1])\n",
    "\n",
    "        clean_label = f\"{name[case]} clean\"\n",
    "        backdoor_label = f\"{name[case]} backdoor\"\n",
    "        clean_labels.append(clean_label)\n",
    "        backdoor_labels.append(backdoor_label)\n",
    "\n",
    "    plt.bar(x_axis-0.2, clean_data, 0.4, label=\"clean\")\n",
    "    plt.bar(x_axis+0.2, backdoor_data, 0.4, label=\"backdoor\")\n",
    "\n",
    "    plt.xticks(x_axis, method_names)\n",
    "    plt.xlabel('Methods')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(\"Last Round Accuracy\")\n",
    "    plt.grid()\n",
    "    plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Last round accuracy\n",
    "\n",
    "\"\"\"\n",
    "    This cell is to run the first experiment: accuracy on the last round before onboarding\n",
    "\"\"\"\n",
    "\n",
    "path = f\"{configs['dataset']}_C5_BS128_R{configs[configs['dataset']]['num_round']}_UR{configs[configs['dataset']]['num_unlearn_round']}_PR{configs[configs['dataset']]['num_post_training_round']}_E1_LR0.01.pkl\"\n",
    "\n",
    "show_last_round_result_before_onboarding(path, methods=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_numerical_result(path, methods=[1, 2, 3, 4], dataset = \"mnist\"):\n",
    "    filename_baseline = f\"case0_{path}\"\n",
    "    baseline = load_gen(folder + filename_baseline)\n",
    "\n",
    "    clean_data = []\n",
    "    backdoor_data = []\n",
    "\n",
    "    clean_labels = []\n",
    "    backdoor_labels = []\n",
    "\n",
    "    # method_names = [name[f\"case{i}\"] for i in methods]\n",
    "    # x_axis = np.arange(len(method_names))\n",
    "\n",
    "    for i in methods:\n",
    "        filename = f\"case{i}_{path}\"\n",
    "        try:\n",
    "            data = load_gen(folder + filename)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        case = f\"case{i}\"\n",
    "\n",
    "        # clean_data.append(data[\"clean\"][-1])\n",
    "        # backdoor_data.append(data[\"backdoor\"][-1])\n",
    "\n",
    "        clean_label = f\"{name[case]} clean\"\n",
    "        # clean_labels.append(clean_label)\n",
    "        # backdoor_labels.append(backdoor_label)\n",
    "\n",
    "        print(clean_label)\n",
    "        \n",
    "        res_str = \"\"\n",
    "\n",
    "        if i == 1:\n",
    "            # i=1: Continue train\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_round'] - 1]} & {data['backdoor'][configs[dataset]['num_round'] - 1]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_round']]} & {data['backdoor'][configs[dataset]['num_round']]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round'] - 1]} & {data['backdoor'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round'] - 1]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round']]} & {data['backdoor'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round']]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round'] - 1]} & {data['backdoor'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round'] - 1]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round']]} & {data['backdoor'][configs[dataset]['num_round'] + configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round']]} & \"\n",
    "            res_str += f\"{data['clean'][-1]} & {data['backdoor'][-1]}\"\n",
    "        else:\n",
    "            res_str += f\"{baseline['clean'][configs[dataset]['num_round'] - 1]} & {baseline['backdoor'][configs[dataset]['num_round'] - 1]} & \"\n",
    "            res_str += f\"{data['clean'][0]} & {data['backdoor'][0]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_unlearn_round'] - 1]} & {data['backdoor'][configs[dataset]['num_unlearn_round'] - 1]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_unlearn_round']]} & {data['backdoor'][configs[dataset]['num_unlearn_round']]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round'] - 1]} & {data['backdoor'][configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round'] - 1]} & \"\n",
    "            res_str += f\"{data['clean'][configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round']]} & {data['backdoor'][configs[dataset]['num_unlearn_round'] + configs[dataset]['num_post_training_round']]} & \"\n",
    "            res_str += f\"{data['clean'][-1]} & {data['backdoor'][-1]}\"\n",
    "\n",
    "\n",
    "        print(res_str)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f\"{configs['dataset']}_C5_BS128_R{configs[configs['dataset']]['num_round']}_UR{configs[configs['dataset']]['num_unlearn_round']}_PR{configs[configs['dataset']]['num_post_training_round']}_E1_LR0.01.pkl\"\n",
    "show_numerical_result(path, methods=[1, 2, 3, 4, 5], dataset = configs['dataset'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_time(filename, type=\"acc\"):\n",
    "    with open(filename, 'rb') as fp:\n",
    "        data = pickle.load(fp)[\"time\"]\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "def show_time(path, methods):\n",
    "    num_rounds = 0\n",
    "    for i in [3, 4, 5]:\n",
    "        temp = 0\n",
    "        if i == 3:\n",
    "            temp = int(path.split(\"_\")[i][1:])\n",
    "        else:\n",
    "            temp = int(path.split(\"_\")[i][2:])\n",
    "        num_rounds += temp\n",
    "\n",
    "    num_rounds += num_onboarding_rounds\n",
    "\n",
    "    fl_rounds = [i for i in range(1, num_rounds + 1)]\n",
    "\n",
    "    method_names = [name[f\"case{i}\"] for i in methods]\n",
    "    x_axis = np.arange(len(method_names))\n",
    "\n",
    "    retrain_time = 0\n",
    "    factors = []\n",
    "\n",
    "    for i in methods:\n",
    "        filename = f\"case{i}_{path}\"\n",
    "        try:\n",
    "            time = load_time(folder + filename)\n",
    "        except:\n",
    "            print(filename)\n",
    "            continue\n",
    "\n",
    "        if i == 1:\n",
    "            retrain_time = time\n",
    "\n",
    "        factor = time/retrain_time\n",
    "\n",
    "        factors.append(factor)\n",
    "\n",
    "    # print(method_names)\n",
    "    # print(factors)\n",
    "    plt.bar(method_names, factors)\n",
    "    plt.ylabel('Unit')\n",
    "    plt.grid()\n",
    "    #plt.locator_params(axis=\"x\", integer=True)\n",
    "    #plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Unlearning time\n",
    "\n",
    "\"\"\"\n",
    "    This cell is to run the fifth experiment: measuring unlearning time\n",
    "\"\"\"\n",
    "\n",
    "path = f\"{configs['dataset']}_C5_BS128_R{configs[configs['dataset']]['num_round']}_UR{configs[configs['dataset']]['num_unlearn_round']}_PR{configs[configs['dataset']]['num_post_training_round']}_E1_LR0.01.pkl\"\n",
    "show_time(path, methods=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_time_detail(path, methods):\n",
    "    num_rounds = 0\n",
    "    for i in [3, 4, 5]:\n",
    "        temp = 0\n",
    "        if i == 3:\n",
    "            temp = int(path.split(\"_\")[i][1:])\n",
    "        else:\n",
    "            temp = int(path.split(\"_\")[i][2:])\n",
    "        num_rounds += temp\n",
    "\n",
    "    num_rounds += num_onboarding_rounds\n",
    "\n",
    "    fl_rounds = [i for i in range(1, num_rounds + 1)]\n",
    "\n",
    "    method_names = [name[f\"case{i}\"] for i in methods]\n",
    "    x_axis = np.arange(len(method_names))\n",
    "\n",
    "    retrain_time = 0\n",
    "    factors = []\n",
    "\n",
    "    for i in methods:\n",
    "        filename = f\"case{i}_{path}\"\n",
    "        try:\n",
    "            time = load_time(folder + filename)\n",
    "        except:\n",
    "            print(filename)\n",
    "            continue\n",
    "\n",
    "        if i == 1:\n",
    "            retrain_time = time\n",
    "\n",
    "        factor = time/retrain_time\n",
    "\n",
    "        factors.append(factor)\n",
    "\n",
    "        print(f\"{method_names[i-1]}: {time:.2f}({(retrain_time / time):.2f}x)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Unlearning time\n",
    "\n",
    "\"\"\"\n",
    "    This cell is to run the fifth experiment: measuring unlearning time\n",
    "\"\"\"\n",
    "\n",
    "path = f\"{configs['dataset']}_C5_BS128_R{configs[configs['dataset']]['num_round']}_UR{configs[configs['dataset']]['num_unlearn_round']}_PR{configs[configs['dataset']]['num_post_training_round']}_E1_LR0.01.pkl\"\n",
    "show_time_detail(path, methods=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"dataset\": configs['dataset'],\n",
    "    \"num_clients\": 5,\n",
    "    \"batch_size\": 128,\n",
    "    \"num_rounds\": configs[configs['dataset']]['num_round'],\n",
    "    \"num_unlearn_rounds\": configs[configs['dataset']]['num_unlearn_round'],\n",
    "    \"num_post_training_rounds\": configs[configs['dataset']]['num_post_training_round'],\n",
    "    \"local_epochs\": 1,\n",
    "    \"lr\": 0.01,\n",
    "    \"device\": torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    \"poisoned_percent\": 0.9\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    model = get_model(args, plotting=True)\n",
    "    model.load_state_dict(torch.load(path))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the baseline model after learning phase\n",
    "case = 2\n",
    "\n",
    "path = f\"../results/models/case1/case1_{args['dataset']}_C{args['num_clients']}_BS{args['batch_size']}_R{args['num_rounds']}_UR{args['num_unlearn_rounds']}_PR{args['num_post_training_rounds']}_E{args['local_epochs']}_LR{args['lr']}_round{args['num_rounds'] - 1}.pt\"\n",
    "baseline_model = load_model(path)\n",
    "\n",
    "# path2 = f\"../results/models/case{case}/case{case}_{args['dataset']}_C{args['num_clients']}_BS{args['batch_size']}_R{args['num_rounds']}_UR{args['num_unlearn_rounds']}_PR{args['num_post_training_rounds']}_E{args['local_epochs']}_LR{args['lr']}_round{args['num_rounds']}.pt\"\n",
    "# model2 = load_model(path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_diff(X, Y, title):\n",
    "    Y = [y.cpu().numpy() for y in Y]\n",
    "    \n",
    "    plt.plot(X, Y)\n",
    "\n",
    "    plt.xlabel('Rounds')\n",
    "    plt.ylabel('Difference')\n",
    "\n",
    "    plt.xticks(np.arange(min(X), max(X)+1, len(X) // 10))\n",
    "    \n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_prediction(model1, model2, data_loader):\n",
    "    model1.eval()\n",
    "    model2.eval()\n",
    "\n",
    "    output1s = torch.tensor([])\n",
    "    output2s = torch.tensor([])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data = data.to(args[\"device\"])\n",
    "            target = target.to(args[\"device\"])\n",
    "\n",
    "            output1 = model1(data).argmax(dim=1).detach().cpu().float()\n",
    "            output2 = model2(data).argmax(dim=1).detach().cpu().float()\n",
    "\n",
    "            output1s = torch.cat((output1s, output1))\n",
    "            output2s = torch.cat((output2s, output2))\n",
    "    \n",
    "    \n",
    "    cos = torch.nn.CosineSimilarity(dim=0, eps=1e-9)\n",
    "    return cos(output1s, output2s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataloader import get_loaders\n",
    "train_loaders, test_loader, test_loader_poison = get_loaders(args, plotting=True)\n",
    "\n",
    "\n",
    "markers = [\"\", \"\", \"^\", \"s\", \"<\", \"o\", \"v\"]\n",
    "colors = [\"\", \"\", \"b\", \"orange\", \"g\", \"r\", \"k\"]\n",
    "\n",
    "for case in [2,3,4,5]:\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for i in range(args['num_rounds'], args['num_rounds'] + args['num_unlearn_rounds'] + args['num_post_training_rounds']):\n",
    "        path = f\"../results/models/case{case}/case{case}_{args['dataset']}_C{args['num_clients']}_BS{args['batch_size']}_R{args['num_rounds']}_UR{args['num_unlearn_rounds']}_PR{args['num_post_training_rounds']}_E{args['local_epochs']}_LR{args['lr']}_round{i}.pt\"\n",
    "        unlearned_model = load_model(path)\n",
    "\n",
    "        cos_sim = compare_prediction(unlearned_model, baseline_model, test_loader)\n",
    "        # print(cos_sim)\n",
    "        X.append(i)\n",
    "        Y.append(cos_sim)\n",
    "\n",
    "    \n",
    "    Y = [y.cpu().numpy() for y in Y]\n",
    "    \n",
    "    case_name = f\"case{case}\"\n",
    "\n",
    "    plt.plot(X, Y, marker = markers[case], markevery= 10, color = colors[case], label=f\"{name[case_name]}\")\n",
    "    \n",
    "\n",
    "plt.xticks(np.arange(min(X), max(X)+1, len(X) // 10))\n",
    "\n",
    "plt.xlabel('Rounds')\n",
    "plt.ylabel('Cosine Similarity')\n",
    "\n",
    "plt.grid()\n",
    "plt.legend(loc='best')\n",
    "\n",
    "\n",
    "plt.savefig(f\"plot/{configs['dataset']}/Cosine_Similarity.png\", dpi=1200, bbox_inches='tight')\n",
    "plt.savefig(f\"plot/{configs['dataset']}/Cosine_Similarity.pdf\", dpi=1200, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
